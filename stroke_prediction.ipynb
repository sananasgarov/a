{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc7bdc5",
   "metadata": {},
   "source": [
    "# Stroke Prediction (Logistic Regression from Scratch)\n",
    "\n",
    "Bu notebook Google Colab istifadəsi üçün nəzərdə tutulub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Import libraries\n",
    "# Google Colab-da fayl yükləmək üçün bu hissəni istifadə edə bilərsiniz.\n",
    "# İlk dəfə işə salanda 'a.json' faylını seçin.\n",
    "try:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "except ImportError:\n",
    "    print(\"Google Colab mühitində deyilsiniz və ya lokal mühitdəsiniz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load & explore data\n",
    "# Fayl adı 'stroke_data.csv' olduğu üçün onu oxuyuruq\n",
    "try:\n",
    "    df = pd.read_csv('stroke_data.csv')\n",
    "except Exception as e:\n",
    "    print(f\"Fayl oxunmadi: {e}\")\n",
    "\n",
    "print(\"Data Head:\")\n",
    "display(df.head()) # Colab-da display() daha yaxşı görünür\n",
    "print(\"\\nData Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ea7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data preprocessing / feature engineering\n",
    "\n",
    "# Handling Missing Values\n",
    "# 'bmi' column has 'N/A' strings in the preview. Let's force it to numeric and handle NaNs.\n",
    "df['bmi'] = pd.to_numeric(df['bmi'], errors='coerce')\n",
    "df['bmi'] = df['bmi'].fillna(df['bmi'].mean())\n",
    "\n",
    "# DROP ID as it's not a feature\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop('id', axis=1)\n",
    "\n",
    "# Encoding Categorical Variables\n",
    "# We will use one-hot encoding.\n",
    "# dtype=int ensures we get 0/1 instead of False/True which is better for numerical operations\n",
    "df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "\n",
    "# Feature Scaling (Crucial for Gradient Descent from scratch)\n",
    "# We will scale all columns except the target 'stroke'\n",
    "target = 'stroke'\n",
    "features = [c for c in df.columns if c != target]\n",
    "\n",
    "X = df[features].values.astype(float)\n",
    "y = df[target].values\n",
    "\n",
    "# Normalize features: (x - mean) / std\n",
    "# Using Standardization (Z-score normalization)\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / (std + 1e-8) # Add small epsilon to avoid division by zero\n",
    "\n",
    "# Add intercept term (column of 1s) to X\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "print(\"Preprocessing done. X shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train/Test split\n",
    "def train_test_split_scratch(X, y, test_size=0.2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    test_samples = int(X.shape[0] * test_size)\n",
    "    \n",
    "    test_indices = indices[:test_samples]\n",
    "    train_indices = indices[test_samples:]\n",
    "    \n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_scratch(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ec4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Choose model (Logistic Regression from Scratch)\n",
    "# Since the target 'stroke' is 0 or 1, this is a classification problem.\n",
    "\n",
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.losses = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            # Linear combination\n",
    "            linear_model = np.dot(X, self.weights)\n",
    "            # Activation\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            \n",
    "            # Compute loss (optional, for monitoring)\n",
    "            # Binary Cross Entropy\n",
    "            epsilon = 1e-15\n",
    "            y_pred_clipped = np.clip(y_predicted, epsilon, 1 - epsilon)\n",
    "            loss = - (1/n_samples) * np.sum(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                # Colab-da print çox düşməsin deyə 500-dən bir də yaza bilərik\n",
    "                pass # print(f\"Iteration {i}: Loss {loss}\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.weights)\n",
    "        return self.sigmoid(linear_model)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        y_predicted_cls = [1 if i > threshold else 0 for i in self.predict_proba(X)]\n",
    "        return np.array(y_predicted_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eeaa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train model\n",
    "print(\"Training Logistic Regression from scratch...\")\n",
    "model = LogisticRegressionScratch(learning_rate=0.1, iterations=2000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(model.losses)\n",
    "plt.title(\"Loss over iterations\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05066894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Predict\n",
    "print(\"Predicting on test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 8. Evaluate (Accuracy, Precision, Recall, F1)\n",
    "def evaluate(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    if (tp + fp) > 0:\n",
    "        precision = tp / (tp + fp)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "        \n",
    "    if (tp + fn) > 0:\n",
    "        recall = tp / (tp + fn)\n",
    "    else:\n",
    "        recall = 0.0\n",
    "        \n",
    "    if (precision + recall) > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "        \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate(y_test, y_pred)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
